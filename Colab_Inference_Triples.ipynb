{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Colab Inference : Triples",
      "provenance": [],
      "collapsed_sections": [
        "PQcKXNcYb-kO",
        "siH0EUgEcLej",
        "u_OB6Da_eF0e",
        "rznh0cJedlvB"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "36d73c76f08846a6b49491c580b750a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_de29e6496a324dd5a38f1e005075b594",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5910dca902b141679514523ea1c5c59b",
              "IPY_MODEL_7e16080a1a7143b89f55d08ae31fc20c"
            ]
          }
        },
        "de29e6496a324dd5a38f1e005075b594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5910dca902b141679514523ea1c5c59b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4040b9154ac64b37b2c41ef74f585026",
            "_dom_classes": [],
            "description": "inference iterations:  10%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 620,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 62,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60fb1efdda7b467ba7983c9140ac5944"
          }
        },
        "7e16080a1a7143b89f55d08ae31fc20c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e70bee6a7e8b4ea19ba3ab55352a4e8e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 62/620 [00:40&lt;06:02,  1.54batch/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ea6ad044b7944d58d36c93738546de1"
          }
        },
        "4040b9154ac64b37b2c41ef74f585026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60fb1efdda7b467ba7983c9140ac5944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e70bee6a7e8b4ea19ba3ab55352a4e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ea6ad044b7944d58d36c93738546de1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQcKXNcYb-kO"
      },
      "source": [
        "# One-timers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDktLi3sb_44",
        "outputId": "e4f91864-27b3-4001-ceca-bc2fdd4502e2"
      },
      "source": [
        "!nvidia-smi\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!mkdir dataset\n",
        "\n",
        "!cp /content/drive/MyDrive/Research/triples/data/*.csv dataset\n",
        "!cp -r /content/drive/MyDrive/Research/triples/HuggingFace dataset\n",
        "!ls dataset\n",
        "!pip3 install -q transformers tensorboard_logger seqeval sentencepiece tokenizers sentence_transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jul 27 12:56:59 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Mounted at /content/drive\n",
            "mkdir: cannot create directory â€˜datasetâ€™: File exists\n",
            "20NewsGroups.csv  most_frequent.csv  squad_frequents.csv  triples.csv\n",
            "frequents.csv\t  results.csv\t     squad_train.csv\n",
            "HuggingFace\t  squad1.csv\t     squad_triples.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siH0EUgEcLej"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDFduBq7cGAd"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
        "import time, torch, random, glob, re, gc, datetime, tokenizers, pdb, errno\n",
        "\n",
        "import numpy as np\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tokenizers import *\n",
        "from transformers import *\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.nn import functional as F\n",
        "from itertools import cycle, chain\n",
        "from torch.utils.data import Dataset, DataLoader, IterableDataset, TensorDataset\n",
        "# from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split, RepeatedKFold, KFold\n",
        "from ast import literal_eval as eval\n",
        "\n",
        "# from transformers import RobertaForSequenceClassification, RobertaConfig, RobertaTokenizer, RobertaForTokenClassification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk9_g3N1cU4A"
      },
      "source": [
        "import sys\n",
        "DRIVE_DIR=\"/content/drive/My Drive/Research/triples/\"\n",
        "sys.path.insert(0, DRIVE_DIR)\n",
        "from utils import seed_everything, count_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXUHopxMcn06"
      },
      "source": [
        "# Globals and Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68syWH6Vcm_C"
      },
      "source": [
        "class Config:\n",
        "    random_state=2021\n",
        "    k=5\n",
        "    device=\"cuda\"\n",
        "    seed = 2021\n",
        "    model=\"bert-base-cased\"\n",
        "    checkpoints=[f\"/content/drive/MyDrive/Research/triples/2021-07-24/NER_bert-base-cased_fold-{i+1}_epoch-12_.pt\" for i in range(k)]\n",
        "    pretrained=True\n",
        "    lowercase = False\n",
        "    task=\"NER\"\n",
        "    num_labels=4\n",
        "    batch_size = 256\n",
        "    weight_decay =0.001\n",
        "    max_len=128\n",
        "    \n",
        "CP_DIR=Path(\"/content/drive/MyDrive/Research/triples\")\n",
        "NUM_WORKERS = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhDM6_XFrqhc",
        "outputId": "f209781c-db5b-4b0c-8be1-0a02d6823536"
      },
      "source": [
        "Config.checkpoints"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Research/triples/2021-07-24/NER_bert-base-cased_fold-1_epoch-12_.pt',\n",
              " '/content/drive/MyDrive/Research/triples/2021-07-24/NER_bert-base-cased_fold-2_epoch-12_.pt',\n",
              " '/content/drive/MyDrive/Research/triples/2021-07-24/NER_bert-base-cased_fold-3_epoch-12_.pt',\n",
              " '/content/drive/MyDrive/Research/triples/2021-07-24/NER_bert-base-cased_fold-4_epoch-12_.pt',\n",
              " '/content/drive/MyDrive/Research/triples/2021-07-24/NER_bert-base-cased_fold-5_epoch-12_.pt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Y2kcgzcp3B"
      },
      "source": [
        "TRANSFORMERS_DIR=Path(\"dataset/HuggingFace/\")\n",
        "TRANSFORMERS={\n",
        "    \"roberta-base\":{\n",
        "        \"model_config\":(RobertaModel, RobertaConfig),\n",
        "        \"tokenizer\":RobertaTokenizer,\n",
        "    },\n",
        "    \"bert-base-cased\":{\n",
        "        \"model_config\":(BertModel, BertConfig),\n",
        "        \"tokenizer\":BertWordPieceTokenizer,\n",
        "    },\n",
        "    \"bert-base-uncased\":{\n",
        "        \"model_config\":(BertModel, BertConfig),\n",
        "        \"tokenizer\":BertWordPieceTokenizer,\n",
        "    },\n",
        "    \"albert-base-v2\":{\n",
        "        \"model_config\":(AlbertModel,AlbertConfig),\n",
        "        \"tokenizer\":AlbertTokenizer,\n",
        "    },\n",
        "    \"gpt2\":{\n",
        "        \"model_config\":(GPT2Model, GPT2Config),\n",
        "        \"tokenizer\":GPT2Tokenizer,\n",
        "    },\n",
        "    \"distilbert-base-cased\":{\n",
        "        \"model_config\":(DistilBertModel, DistilBertConfig),\n",
        "        \"tokenizer\":DistilBertTokenizer,\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_OB6Da_eF0e"
      },
      "source": [
        "# Function and Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tty9esMHeH5_"
      },
      "source": [
        "def load(model, with_checkpoint=None):\n",
        "    model=Transformer(model)\n",
        "    if with_checkpoint:\n",
        "        checkpoint=torch.load(with_checkpoint, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint)\n",
        "        print(\"Checkpoint loaded!\", end=\"\\r\")\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rznh0cJedlvB"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_f32rfDc2YJ"
      },
      "source": [
        "# class TestDataset(Dataset):\n",
        "#     def __init__(self, df, tokenizer, tokens, max_len=128, model_name=\"bert\", task=\"qa\"):\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.tokens = tokens\n",
        "#         self.max_len = max_len\n",
        "#         try:\n",
        "#             processed = df.processed.map(eval)\n",
        "#         except:\n",
        "#             processed = df.processed.map\n",
        "\n",
        "#         self.sentences = list(chain.from_iterable(processed))\n",
        "#         row_number = []\n",
        "#         for idx, item in enumerate(df.num_sentences.values):\n",
        "#             row_number.extend([idx] * item)\n",
        "#         self.row_number = row_number\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.sentences)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         data = self.sentences[idx]\n",
        "#         tokenized = self.tokenizer.encode(data)\n",
        "#         input_ids = tokenized.ids\n",
        "#         offsets = tokenized.offsets\n",
        "\n",
        "#         return {\n",
        "#             \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "#             \"offsets\": offsets,\n",
        "#             \"data\": data,\n",
        "#         }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMpSxtyudmAp"
      },
      "source": [
        "class TestDataset(IterableDataset):\n",
        "    def __init__(self, df, tokenizer, tokens, max_len=16):\n",
        "        self.tokenizer=tokenizer\n",
        "        self.tokens=tokens\n",
        "        self.max_len=max_len\n",
        "        try:\n",
        "            processed=df.processed.map(eval)\n",
        "        except:\n",
        "            processed=df.processed\n",
        "            \n",
        "        self.sentences=list(chain.from_iterable(processed))\n",
        "        row_number=[]\n",
        "        for idx, item in enumerate(df.num_sentences.values):\n",
        "            row_number.extend([idx]*item)\n",
        "        self.row_number=row_number\n",
        "        \n",
        "        self.ids_=list(range(len(self.sentences)))\n",
        "        \n",
        "        self.pad_token=0\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        \"\"\"Inaccurate function for necessity\"\"\"\n",
        "        return len(self.sentences)\n",
        "    \n",
        "    def pad(self, array, pad_token=-2):\n",
        "        \"\"\"Customized padding\"\"\"        \n",
        "        if pad_token==-2:\n",
        "            pad_token=self.pad_token\n",
        "            \n",
        "\n",
        "        end_=len(array)%self.max_len\n",
        "        if end_>0:\n",
        "            full_len=((len(array)//self.max_len)+1)*self.max_len\n",
        "        elif end_==0:\n",
        "            return array\n",
        "        else:\n",
        "            full_len=self.max_len\n",
        "             \n",
        "        newArray=np.full(full_len, pad_token)\n",
        "        end=end_+(len(array)//self.max_len)*self.max_len\n",
        "        newArray[:end]=array\n",
        "        return newArray\n",
        "    \n",
        "    def chunks(self, list_):\n",
        "\n",
        "        \"for an array longer than the maxlen, this function returns a 2d array\"\n",
        "        l=len(list_)\n",
        "        n=self.max_len\n",
        "        if l%n>0:\n",
        "            N=n*((l//n)+1)\n",
        "        else:\n",
        "            N=l-l%n\n",
        "        for i in range(0, N, n):\n",
        "            yield np.array(list_[i:i+n], dtype=\"long\")\n",
        "    \n",
        "    def getitems(self, idx):\n",
        "\n",
        "        data=self.sentences[idx]\n",
        "        tokenized=self.tokenizer.encode(data)\n",
        "        input_ids=tokenized.ids\n",
        "        offsets=tokenized.offsets\n",
        "        offset_start=[i[0] for i in offsets]\n",
        "        offset_end=[i[1] for i in offsets]\n",
        "            \n",
        "#         if len(input_ids)>self.max_len and (len(input_ids)%self.max_len)<(self.max_len*0.5):\n",
        "#             input_ids=input_ids[:self.max_len]\n",
        "#             input_ids[-1]=102\n",
        "            \n",
        "        if len(input_ids)==self.max_len:\n",
        "            input_ids=torch.LongTensor([input_ids])\n",
        "            offset_start=torch.LongTensor([offset_start])\n",
        "            offset_end=torch.LongTensor([offset_end])\n",
        "            \n",
        "        elif len(input_ids)<self.max_len:\n",
        "            input_ids=self.pad(input_ids)\n",
        "            offset_start=self.pad(offset_start, -1)\n",
        "            offset_end=self.pad(offset_end, -1)\n",
        "            input_ids=torch.LongTensor([input_ids])\n",
        "            offset_start=torch.LongTensor([offset_start])\n",
        "            offset_end=torch.LongTensor([offset_end])\n",
        "            \n",
        "        else:\n",
        "            temp1=self.max_len-(len(input_ids)%self.max_len)\n",
        "            input_ids.extend([0]*temp1)\n",
        "            offset_start.extend([-1]*temp1)\n",
        "            offset_end.extend([-1]*temp1)\n",
        "            input_ids=torch.LongTensor(list(self.chunks(input_ids)))\n",
        "            offset_start=torch.LongTensor(list(self.chunks(offset_start)))            \n",
        "            offset_end=torch.LongTensor(list(self.chunks(offset_end)))\n",
        "\n",
        "        shape=offset_start.shape\n",
        "        \n",
        "        for idx_, btch in enumerate(input_ids):\n",
        "            yield {\n",
        "                \"data\":data,\n",
        "                \"input_ids\":torch.as_tensor(btch, dtype=torch.long),\n",
        "                \"offsets_start\":torch.as_tensor(offset_start[idx_], dtype=torch.long),\n",
        "                \"offsets_end\":torch.as_tensor(offset_end[idx_],dtype=torch.long),\n",
        "                \"row_number\":torch.as_tensor(self.row_number[idx])\n",
        "            }\n",
        "\n",
        "    def getStream(self, ids):\n",
        "        yield from chain.from_iterable(map(self.getitems, ids))\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.getStream(self.ids_)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmbJXpnddo-E"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2x6XWQ-dpvD"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  \n",
        "    def __init__(self, model, maxlen=128):\n",
        "        super().__init__()\n",
        "        self.name = model\n",
        "        model_type, config_type=TRANSFORMERS[model]['model_config']\n",
        "        if Config.pretrained:\n",
        "            self.transformer=model_type.from_pretrained(model, output_hidden_states=True, num_labels=Config.num_labels)\n",
        "        else:\n",
        "            config_file=TRANSFORMERS[model]['config']\n",
        "            config=config_type.from_json_file(config_file)\n",
        "            config.num_labels=Config.num_labels\n",
        "            config.output_hidden_states=True\n",
        "            self.transformer=model_type(config)\n",
        "            \n",
        "        self.nb_features = self.transformer.pooler.dense.out_features\n",
        "        if \"roberta\" in self.name:\n",
        "            self.pad_idx=1\n",
        "        else:\n",
        "            self.pad_idx=0\n",
        "        self.logits = nn.Sequential(\n",
        "            nn.Linear(self.nb_features, self.nb_features),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.nb_features, Config.num_labels),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):        \n",
        "        hidden_states = self.transformer(\n",
        "            input_ids,\n",
        "            attention_mask=(input_ids != self.pad_idx).long(),\n",
        "        )[-1]\n",
        "\n",
        "        features = hidden_states[-1]\n",
        "        logits = torch.sigmoid(self.logits(features))\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB5Fvq41eXCX"
      },
      "source": [
        "# Fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjETCrIrdsMM"
      },
      "source": [
        "def fit(model,dataset, batch_size, first_time=False):\n",
        "\n",
        "  \"\"\"Fits in batches and returns the logits\"\"\"\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=NUM_WORKERS)\n",
        "\n",
        "    #just the streaming dataset thing\n",
        "    if batch_size==256:\n",
        "      dataloader_len=620\n",
        "    else:\n",
        "      for idx, _ in enumerate(dataloader):\n",
        "        pass\n",
        "      dataloader_len=idx+1\n",
        "\n",
        "    model.to(Config.device)\n",
        "    all_logits=[]\n",
        "    start_indices=[]\n",
        "    end_indices=[]\n",
        "    rows=[]\n",
        "    texts=[]\n",
        "    with torch.no_grad():\n",
        "        with tqdm(total=dataloader_len, desc=\"inference iterations\", unit=\"batch\", position=1, leave=True) as pbar:\n",
        "            for idx, data in enumerate(dataloader):\n",
        "              input_ids=data['input_ids']\n",
        "              logits=model(input_ids=input_ids.to(Config.device))\n",
        "              all_logits.append(logits.detach().cpu())\n",
        "\n",
        "              if first_time:\n",
        "                start=data['offsets_start'].detach().cpu()\n",
        "                end=data['offsets_end'].detach().cpu()\n",
        "                row=data['row_number'].detach().cpu()\n",
        "                text=data['data']\n",
        "\n",
        "\n",
        "                start_indices.append(start)\n",
        "                end_indices.append(end)\n",
        "                rows.append(row)\n",
        "                texts.append(text)\n",
        "              pbar.update()\n",
        "\n",
        "    if Config.device != \"cpu\":\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    if first_time:\n",
        "      all_logits= torch.cat(all_logits, dim=0)\n",
        "      start_indices= torch.cat(start_indices)\n",
        "      end_indices=torch.cat(end_indices)\n",
        "      rows=torch.cat(rows)\n",
        "      texts=list(chain.from_iterable(texts))\n",
        "      return all_logits, start_indices, end_indices, rows, texts\n",
        "    else:\n",
        "      return torch.cat(all_logits, dim=0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exVOKdshgaCR"
      },
      "source": [
        "# K-fold inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52ZpeGZgeWKy"
      },
      "source": [
        "def k_fold(df, save=True, config=None):\n",
        "\n",
        "  \"\"\"Calls fit function for each checkpoint and then ensembles the result.\"\"\"\n",
        "    tokenizer = BertWordPieceTokenizer(\n",
        "                \"dataset/HuggingFace/Bert/bert_base_uncased_vocab.txt\",\n",
        "                lowercase=False\n",
        "            )\n",
        "    tokens = {\n",
        "            'cls': tokenizer.token_to_id('[CLS]'),\n",
        "            'sep': tokenizer.token_to_id('[SEP]'),\n",
        "            'pad': tokenizer.token_to_id('[PAD]'),\n",
        "        }\n",
        "\n",
        "    seed_everything(config.seed)\n",
        "    checkpointwise_preds=[]\n",
        "    first_time=True\n",
        "    for idx, checkpoint in enumerate(Config.checkpoints):\n",
        "\n",
        "      score = 0\n",
        "      dataset=TestDataset(df=df, tokenizer=tokenizer, tokens=tokens, max_len=Config.max_len)\n",
        "      model=load(Config.model, with_checkpoint=checkpoint)\n",
        "      # if not model.transformer.config.pad_token_id:\n",
        "      #     model.transformer.config.pad_token_id=tokenizer.eos_token_id\n",
        "\n",
        "      model.eval()\n",
        "      if first_time:\n",
        "        preds, start_indices, end_indices, rows, texts = fit(model, dataset,  batch_size=config.batch_size, first_time=first_time)\n",
        "        first_time=False\n",
        "      else:\n",
        "        preds=fit(model, dataset,  batch_size=config.batch_size, first_time=first_time)\n",
        "      checkpointwise_preds.append(preds)\n",
        "\n",
        "      del model, dataset, preds\n",
        "      if Config.device!=\"cpu\":\n",
        "          torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "    preds=torch.mean(torch.stack(checkpointwise_preds), dim=0)\n",
        "    \n",
        "    return preds, start_indices, end_indices, rows, texts\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVa5bWAZjmCq"
      },
      "source": [
        "# Infer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXwSy9X2hrYt"
      },
      "source": [
        "df=pd.read_csv(\"dataset/20NewsGroups.csv\")\n",
        "df[\"num_sentences\"]=df['processed'].map(eval).map(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413,
          "referenced_widgets": [
            "36d73c76f08846a6b49491c580b750a7",
            "de29e6496a324dd5a38f1e005075b594",
            "5910dca902b141679514523ea1c5c59b",
            "7e16080a1a7143b89f55d08ae31fc20c",
            "4040b9154ac64b37b2c41ef74f585026",
            "60fb1efdda7b467ba7983c9140ac5944",
            "e70bee6a7e8b4ea19ba3ab55352a4e8e",
            "6ea6ad044b7944d58d36c93738546de1"
          ]
        },
        "id": "4v4UrMHWhxaR",
        "outputId": "e66a9435-9c12-4a22-fa72-c843343d9c2f"
      },
      "source": [
        "folds=k_fold(df,save=True, config=Config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Checkpoint loaded!\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36d73c76f08846a6b49491c580b750a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='inference iterations', max=620.0, style=ProgressStyle(desâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c3634d9c2332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-196c1797bd2d>\u001b[0m in \u001b[0;36mk_fold\u001b[0;34m(df, save, config)\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mfirst_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mfirst_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8ed5301b7ec8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, dataset, batch_size, first_time)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m               \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m               \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m               \u001b[0mall_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-4588c08d4741>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     27\u001b[0m         hidden_states = self.transformer(\n\u001b[1;32m     28\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         )[-1]\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         )\n\u001b[1;32m   1003\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    587\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m                 )\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m         )\n\u001b[1;32m    513\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2180\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNA_MV1kD2MC"
      },
      "source": [
        "def batch_decode(output, offset_s, offset_e, text):\n",
        "  \"\"\"Decodes the predictions back to text using the offsets of the tokenizer\"\"\"\n",
        "    all_results=[]\n",
        "    for batch in range(output.shape[0]):\n",
        "        encoded_decode=dict()\n",
        "        if torch.count_nonzero(output[batch]).item()==0:\n",
        "            continue\n",
        "        out=output[batch].tolist()\n",
        "        idx=0\n",
        "        while idx<len(out):\n",
        "            if out[idx]!=0:\n",
        "                idx2=idx\n",
        "                temp_entity=[]\n",
        "                predicted_class=out[idx2]\n",
        "\n",
        "                while (idx2<len(out)) and (out[idx2]==predicted_class):\n",
        "                    temp_entity.append(idx2)\n",
        "                    idx2+=1\n",
        "\n",
        "                encoded_decode[predicted_class]=[(offset_s[batch][i], offset_e[batch][i]) for i in temp_entity]\n",
        "                \n",
        "            idx+=1\n",
        "            \n",
        "\n",
        "        result=[\" \"]*3\n",
        "        for key in encoded_decode.keys():\n",
        "            res=encoded_decode[key][0]\n",
        "            res=text[0][res[0].item():res[1].item()+1].strip()\n",
        "            result[key-1]=res\n",
        "        if '' in result or \" \" in result or len(set(result))<3:\n",
        "            continue\n",
        "        \n",
        "        if result not in all_results:\n",
        "          all_results.append(result)\n",
        "\n",
        "    return all_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvfR1P-7TqPP"
      },
      "source": [
        "out=folds[0].argmax(2)\n",
        "starts=folds[1]\n",
        "ends=folds[2]\n",
        "rows=folds[3]\n",
        "texts=folds[4]\n",
        "row_wise_results=[]\n",
        "for i in range(max(rows)+1):    \n",
        "    temp=np.argwhere(rows==i).flatten()\n",
        "    out_=out[temp]\n",
        "    start=starts[temp]\n",
        "    end=ends[temp]\n",
        "    texts_=[texts[i] for i in temp]\n",
        "    row_wise_results.append(batch_decode(out_, start, end, texts_))\n",
        "\n",
        "df['bert_triples']=row_wise_results\n",
        "df.head()\n",
        "df[['filenames','data','bert_triples']].to_csv(\"/content/drive/MyDrive/Research/triples/data/results.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntnaLkFdu9zb"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPaVVuUA_Re0"
      },
      "source": [
        "len(df[df['bert_triples'].apply(str)!=\"[]\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}